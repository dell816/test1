 ‚≠ê **Real-World ETL Pipeline Example ‚Äì TD Bank (ADF + Databricks)**

**‚ÄúTell me about an ETL process you built at TD using ADF and Databricks.‚Äù**

In my current role at TD Bank, one of the key ETL pipelines I worked on involved ingesting large volumes of operational and analytical data from on-prem SQL environments into **Azure Data Lake** and transforming it for downstream analytics teams.

### **1. Source Systems & Data Movement**

The pipeline ingested data from:

* **On-prem SQL Server**
* **Internal Cisco telephony datasets**
* **Legacy data repositories used by analytics and QA teams**

The datasets were large, frequently updated, and required consistent schemas.

### **2. Orchestration ‚Äì Azure Data Factory**

I used **Azure Data Factory (ADF)** to orchestrate the end-to-end workflow:

* Scheduled incremental ingestion triggers
* Used copy activities to extract from SQL systems
* Implemented parameterized pipelines so the same pipeline could ingest **multiple datasets**, improving maintainability
* Added **retry logic**, logging, and monitoring alerts to guarantee reliability in production

ADF moved the raw data into **bronze (raw) zones** in the data lake.

### **3. Transformation ‚Äì Azure Databricks & PySpark**

Once the data landed in the lake, **Databricks** handled transformation and quality checks:

* Used **PySpark** to clean, normalize, and merge the data
* Implemented **schema validation** to manage schema drift
* Performed **key transformations** such as:

  * flattening nested structures
  * standardizing timestamps
  * joining reference tables
  * deduplicating large datasets

In one initiative, I developed a **Data Populator Tool (Python + Databricks + Regex)** to automate ingestion preparation, reducing manual transformation time by **~25%**.

### **4. Storage ‚Äì Delta Lake Architecture**

Transformed data was stored in:

* **Bronze** ‚Üí Raw ingestion
* **Silver** ‚Üí Cleaned and validated data
* **Gold** ‚Üí Aggregated or business-ready tables

Delta capabilities like **ACID transactions, time-travel, and optimized Z-ordering** helped with query performance and traceability ‚Äî which is critical for regulated data environments like banking.

### **5. Metadata & Governance**

I also built a **Unified Metadata Search Tool** within Databricks:

* Consolidated metadata from Delta tables, Bitbucket, GitHub, and SQL Server
* Enabled developers and analysts to discover datasets more easily
* Strengthened governance and minimized duplicated pipelines

This tool improved operational efficiency and reduced dependency on tribal knowledge.

### **6. Production Support & Monitoring**

As part of L3 support:

* Investigated ingestion failures
* Resolved schema mismatch issues
* Performed root-cause analysis for broken pipelines
* Documented fixes and updated data lineage

This experience is directly aligned with Deloitte‚Äôs expectations around **data quality, troubleshooting, and surveillance data reliability**.

--------------------------



https://www1.myrbcsso.rbcinsurance.com/rbcquoter/bol/auto/bundle?lang=en&key=WUVmTVk5QTNjYkxwVEREdlBhb3I4cytGQm9UYUd0ZTYvb3hZZ3B1Zm1kRk1uMS9ETExIMWhPUUd4emZRSzlYc0dIdVJJRDBTblgvaVlqK2prTnN2UEpwUkxlaWN2d3JBNXdGNmdsVjJOVjFJTXhpMFYxTmVjTDloUTJLd25oNldwYWZEeU5MTnUzd1Bzc2FmT050THBFSXZSWHlHZndvbXhwUzNXUVpNSUtBPQ

Introduction & Goal of the Session

Hey everyone, thanks for being here.

So today I want to talk about a problem we often face when we‚Äôre looking for table schemas across different places like Bitbucket, DBFS, Azure SQL, or our Delta tables in SRZ. The current process is slow and manual ‚Äî we usually have to open different tools, search by eye, pull data into CSV files, compare them, and even then, sometimes we can‚Äôt find what we‚Äôre looking for. It takes time and can get frustrating.

That‚Äôs why I started thinking: what if we build a tool that can help us with this? The idea is, we give it part of a table name or column name, and it automatically searches across all the data sources we care about, and gives us a list of matches. It should save us a lot of time and manual work.

In this session, I‚Äôll show you the basic idea, a diagram of how it works, and how we can use it.

The only challenge I see for now is access ‚Äî especially when connecting from a Databricks notebook to something like SQL Server. There might be firewalls, tokens, or authentication needed, so we‚Äôll probably need support from other teams like Matthew‚Äôs team to get that sorted out. And if not, we can think of other ways to .





Based on your attached images ‚Äî your feature sheet, the prework Excel, and the workshop invitation ‚Äî here is a strong and structured response to the question:

‚∏ª

Why these projects are meaningful and pivotal to me

These projects represent critical milestones in my professional journey across both TD and the Middle East region. Each one reflects a combination of problem-solving, technical innovation, and leadership.

‚∏ª

üîπ 1. Unified Metadata Search Tool

Why it‚Äôs meaningful:
This tool addresses a core challenge in any large data ecosystem: locating schema, table names, and relationships efficiently.
Message:
I want to highlight my ability to transform pain points into tools that improve collaboration, onboarding, and productivity across teams. This project shows how innovation can begin from a conversation and turn into a tangible solution.

‚∏ª

üîπ 2. IDC Populator (Data Populator Tool)

Why it‚Äôs meaningful:
By leveraging Databricks, Python, and Pandas, I automated a time-consuming ingestion process, reducing delays by 30%.
Message:
This project demonstrates how technical depth can create real-time performance gains at scale, with direct value to the business and platform stability.

‚∏ª

üîπ 3. API Ingestion Migration Project

Why it‚Äôs meaningful:
This project automated ingestion from sources like Salesforce and third-party APIs, replacing time-consuming manual work.
Message:
It reflects my passion for automation, pattern recognition, and designing reusable ingestion frameworks in a scalable way.

‚∏ª

üîπ 4. ERP Development ‚Äì Supply Chain

Why it‚Äôs meaningful:
I designed a full ERP system covering sales, accounting, inventory, routing, and more. This was one of the most complex end-to-end systems I‚Äôve led.
Message:
This shows my ability to build foundational business systems that improve efficiency and support long-term growth. It highlights both technical architecture and strategic thinking.

‚∏ª

üîπ 5. Established IT Department

Why it‚Äôs meaningful:
Built a team from scratch, implemented infrastructure, and developed SOPs. It was a leadership challenge and growth experience.
Message:
This shows my people management skills, department setup, and the ability to deliver systems while building a team culture.

‚∏ª

üîπ 6. ERP Implementation

Why it‚Äôs meaningful:
Executed a massive rollout ‚Äî from infrastructure to training ‚Äî over multiple branches.
Message:
I want to highlight my project execution experience, large-scale rollout planning, and cross-functional coordination.

‚∏ª

üîπ 7. SQL OLTP Design + BI

Why it‚Äôs meaningful:
I designed an OLTP structure integrated with BI, providing performance and reporting improvements.
Message:
It reflects my deep understanding of data modeling, reporting, and supporting business insights through the right backend structure.

‚∏ª

üîπ 8. Data Warehouse + BI

Why it‚Äôs meaningful:
Created a central DW using SSIS and reporting layers for business visibility.
Message:
Demonstrates my expertise in ETL, DW design, and report delivery, especially using Microsoft technologies like SSIS/SSRS.

‚∏ª

üîπ 9. Ordering Mobile App Designer + DB

Why it‚Äôs meaningful:
I led development of a mobile ordering app integrated with the backend for retail and restaurant clients.
Message:
This showcases full-stack thinking and an ability to create intuitive customer-facing solutions driven by backend design.

‚∏ª

üîπ 10. GIS Tracking for Sales/Distributors

Why it‚Äôs meaningful:
Developed a system to track distributor locations and route optimization.
Message:
This is where I began connecting data with real-world logistics, leading into my passion for AI in route optimization and spatial analytics.

‚∏ª

Subject: Thank You for Today‚Äôs Discussion

Hi [Name],

Thank you again for taking the time to speak with me today. I really enjoyed our conversation and learning more about the Data Scientist III role supporting Finance AI2.

I also wanted to mention that I bring direct finance-related experience from my previous ERP project, where I designed core financial modules such as the general ledger, sales, inventory, and financing systems. This background, combined with my technical expertise in big data, ingestion pipelines, Databricks, Python, SQL, and my current work in data analysis within the Data Science team, gives me a strong foundation to support Finance partners effectively.

I‚Äôm very excited about the opportunity to contribute to Finance AI2 and help drive insights, automation, and strong collaboration across the Finance organization.

Thank you again, and please let me know if you need any additional information from me.

Best regards,
Ali
