from pyspark.sql.functions import col

# --- Part 1: Tables that match name keywords --- #
print("=== Part 1: Matching Table Names and Their Columns ===")

for mal in malcodes:
    base_path = f"abfss://{mal}@edaaaedle1sitsrz.dfs.core.windows.net/{mal.upper()}/"
    results = dbutils.fs.ls(base_path)

    for item in results:
        table_name = item.name.rstrip('/')
        full_path = f"{base_path}{table_name}/current/"

        if any(keyword.lower() in table_name.lower() for keyword in table_keywords):
            print(f"\n✅ Table matched: {full_path}")
            df_desc = spark.sql(f"DESCRIBE delta.`{full_path}`") \
                           .filter(~col("col_name").startswith("#")) \
                           .select("col_name", "data_type")
            display(df_desc)

# --- Part 2: Tables that contain column keywords anywhere in their schema --- #
print("\n=== Part 2: Tables that Contain Matching Columns ===")

for mal in malcodes:
    base_path = f"abfss://{mal}@edaaaedle1sitsrz.dfs.core.windows.net/{mal.upper()}/"
    results = dbutils.fs.ls(base_path)

    for item in results:
        table_name = item.name.rstrip('/')
        full_path = f"{base_path}{table_name}/current/"

        df_desc = spark.sql(f"DESCRIBE delta.`{full_path}`") \
                       .filter(~col("col_name").startswith("#")) \
                       .select("col_name", "data_type")

        matched_cols = df_desc.filter(
            col("col_name").rlike("|".join([f".*{c}.*" for c in column_keywords]))
        )

        if matched_cols.count() > 0:
            print(f"\n✅ Table has matching column(s): {full_path}")
            display(df_desc)
